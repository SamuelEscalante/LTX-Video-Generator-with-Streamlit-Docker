import streamlit as st

def arquitectura():
    st.title("ðŸ§  Arquitectura del Modelo LTX Video")

    st.markdown("""
    El modelo **LTX Video**, desarrollado por [Lightricks](https://github.com/Lightricks/LTX-Video), es un sistema de generaciÃ³n de video condicional a texto que se basa en una arquitectura de tipo **Latent Diffusion Model (LDM)** optimizada para generar mÃºltiples fotogramas de forma coherente.

    ### ðŸ§© Componentes clave:

    - **Codificador de texto (CLIP o T5)**: procesa el *prompt* del usuario y lo transforma en una representaciÃ³n latente que se usa como condiciÃ³n para generar el video.
    - **Autoencoder**: convierte los fotogramas del video en una representaciÃ³n latente compacta, acelerando el proceso de difusiÃ³n.
    - **U-Net con atenciÃ³n**: red neuronal central que predice cÃ³mo debe evolucionar el ruido latente hacia un video realista.
    - **Scheduler de difusiÃ³n**: controla el proceso de denoising para generar mÃºltiples frames coherentes en el tiempo.

    ### ðŸ› ï¸ Flujo de trabajo general

    1. El texto se convierte en embeddings (vector de caracterÃ­sticas).
    2. Se inicia un tensor latente con ruido.
    3. El modelo difunde ese ruido en pasos sucesivos condicionados por el texto.
    4. Al final, se decodifica ese tensor a video.

    ### ðŸ–¼ï¸ Diagrama de Arquitectura

    ![Arquitectura LTX](https://github.com/Lightricks/LTX-Video/raw/main/docs/_static/architecture_diagram.png)

    > ðŸ“˜ Imagen tomada del [repositorio oficial de LTX Video](https://github.com/Lightricks/LTX-Video)

    ### ðŸ§ª Enlace al paper (si disponible)

    Actualmente no hay un paper formal publicado. Puedes revisar los detalles tÃ©cnicos directamente en su [GitHub oficial](https://github.com/Lightricks/LTX-Video).

    ---
    """)
